import time
import chromadb
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datasets import load_dataset  # <-- A MÃGICA ESTÃ AQUI

# --- ConfiguraÃ§Ãµes ---
# Vamos usar a WikipÃ©dia em PortuguÃªs!
# VocÃª pode ver outras aqui: https://huggingface.co/datasets/wikipedia
NOME_DATASET_HF = "wikipedia"
CONFIG_DATASET_HF = "20220301.pt"  # 'pt' para portuguÃªs, 'en' para inglÃªs

NOME_MODELO_EMBEDDING = 'paraphrase-multilingual-MiniLM-L12-v2'
NOME_COLECAO = "fatos_bot"
CAMINHO_DB = "./chroma_db"

# Limite de artigos para processar (para nÃ£o rodar para sempre no teste)
# Coloque None para processar a WikipÃ©dia INTEIRA (milhÃµes de artigos, pode levar dias!)
LIMITE_DE_ARTIGOS = 2000  # Comece com um nÃºmero pequeno para testar

# --- ConfiguraÃ§Ãµes do "Fatiador" (Chunker) ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)

print("ðŸ¤– [Ingestor Wiki] Iniciando script de ingestÃ£o...")

# --- 1. Carregar Modelo de Embedding ---
print(f"ðŸ¤– [Ingestor Wiki] Carregando modelo '{NOME_MODELO_EMBEDDING}'...")
try:
    model = SentenceTransformer(NOME_MODELO_EMBEDDING)
    print("ðŸ¤– [Ingestor Wiki] Modelo carregado.")
except Exception as e:
    print(f"ðŸš¨ ERRO ao carregar modelo: {e}")
    exit()

# --- 2. Conectar e Limpar o ChromaDB ---
print(f"ðŸ¤– [Ingestor Wiki] Conectando ao ChromaDB em '{CAMINHO_DB}'...")
try:
    client = chromadb.PersistentClient(path=CAMINHO_DB)

    # Apaga a coleÃ§Ã£o antiga, se existir
    try:
        if NOME_COLECAO in [c.name for c in client.list_collections()]:
            print(f"ðŸ¤– [Ingestor Wiki] ColeÃ§Ã£o '{NOME_COLECAO}' antiga encontrada. Removendo...")
            client.delete_collection(name=NOME_COLECAO)
    except Exception as e:
        print(f"ðŸš¨ ERRO ao tentar apagar coleÃ§Ã£o antiga: {e}")

    collection = client.get_or_create_collection(
        name=NOME_COLECAO,
        metadata={"hnsw:space": "cosine"}
    )
    print(f"ðŸ¤– [Ingestor Wiki] ColeÃ§Ã£o '{NOME_COLECAO}' pronta.")
except Exception as e:
    print(f"ðŸš¨ ERRO ao conectar ao ChromaDB: {e}")
    exit()

# --- 3. Carregar e Processar o Dataset da WikipÃ©dia ---
print(f"ðŸ¤– [Ingestor Wiki] Carregando dataset '{NOME_DATASET_HF}' ({CONFIG_DATASET_HF})...")
print("Isso pode baixar alguns GBs na primeira vez que vocÃª rodar.")

try:
    # streaming=True Ã© crucial! Ele nÃ£o baixa tudo, ele "puxa" sob demanda.
    dataset = load_dataset(NOME_DATASET_HF, CONFIG_DATASET_HF, split='train', streaming=True)

    print("ðŸ¤– [Ingestor Wiki] Dataset carregado. Iniciando processamento...")

    total_chunks = 0
    artigos_processados = 0

    # Prepara lotes (batches) para adicionar ao ChromaDB
    # Isso Ã© MUITO mais rÃ¡pido do que adicionar 1 por 1
    lote_chunks = []
    lote_metadados = []
    lote_ids = []
    TAMANHO_LOTE = 100

    # Itera sobre o dataset (artigo por artigo)
    for doc in dataset:
        if LIMITE_DE_ARTIGOS is not None and artigos_processados >= LIMITE_DE_ARTIGOS:
            print(f"ðŸ¤– [Ingestor Wiki] Atingido o limite de {LIMITE_DE_ARTIGOS} artigos.")
            break

        artigos_processados += 1
        titulo_artigo = doc['title']
        texto_artigo = doc['text']

        # 1. Fatiar (Chunking)
        chunks = text_splitter.split_text(texto_artigo)

        for i, chunk in enumerate(chunks):
            total_chunks += 1
            lote_chunks.append(chunk)
            lote_metadados.append({"fonte": "wikipedia", "titulo": titulo_artigo})
            lote_ids.append(f"wiki_{doc['id']}_chunk_{i}")

            # Quando o lote estiver cheio, processe e adicione ao DB
            if len(lote_chunks) >= TAMANHO_LOTE:
                # 2. Gerar Embeddings (em lote)
                embeddings = model.encode(lote_chunks, show_progress_bar=False).tolist()

                # 3. Adicionar ao ChromaDB (em lote)
                collection.add(
                    embeddings=embeddings,
                    documents=lote_chunks,
                    metadatas=lote_metadados,
                    ids=lote_ids
                )
                print(f"  -> Lote processado. Total de chunks indexados: {total_chunks}")

                # Limpar lotes
                lote_chunks = []
                lote_metadados = []
                lote_ids = []

    # Processar o Ãºltimo lote restante
    if lote_chunks:
        embeddings = model.encode(lote_chunks, show_progress_bar=False).tolist()
        collection.add(
            embeddings=embeddings,
            documents=lote_chunks,
            metadatas=lote_metadados,
            ids=lote_ids
        )
        print(f"  -> Lote final processado. Total de chunks indexados: {total_chunks}")

    print("\nâœ… SUCESSO! Banco de dados vetorial populado com a WikipÃ©dia.")
    print(f"Foram processados {artigos_processados} artigos.")
    print(f"Foram adicionados {collection.count()} chunks Ã  coleÃ§Ã£o '{NOME_COLECAO}'.")

except Exception as e:
    print(f"ðŸš¨ ERRO durante o processamento do dataset: {e}")
    import traceback

    traceback.print_exc()